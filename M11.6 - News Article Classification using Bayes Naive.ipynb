{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This may get a bit hefty for Notepad, but we'll see.\n",
    "# We'll control the counts from here:\n",
    "article_count = 50\n",
    "word_count    = 25\n",
    "num_k         = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ignore_list = set( stopwords.words('english')\n",
    "                  + list(punctuation) \n",
    "                  + ['’',\"'s\",\"'it\",\"'the\",\"‘\",\"'i\",\"n't\",'“','”','–','–','•','…','—'] \n",
    "                  + ['i','we','one','two','1','2','3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_articles( link_prefix, link_list, extract_function, source_name, count ):\n",
    "    processed = 0\n",
    "    article_words = []\n",
    "    for article_href in link_list:\n",
    "        try:\n",
    "            body_paragraphs = extract_function( link_prefix, article_href )\n",
    "            if not body_paragraphs:\n",
    "                raise ValueError('No body content found')\n",
    "            article_blocks  = []\n",
    "            for body in body_paragraphs:\n",
    "                article_blocks.append( \" \".join([ p.text.replace(u'\\xa0', '') for p in body ]) )\n",
    "            \n",
    "            article_text = \" \".join( article_blocks )\n",
    "            word_freq = count_words( article_text, ignore_list )\n",
    "            top_words = top_frequencies( word_count, word_freq, 0.9, 0.1 )\n",
    "            \n",
    "            article_words.append({ 'source':source_name, 'words': tuple(top_words), 'href': article_href })\n",
    "            processed += 1\n",
    "            \n",
    "        except:\n",
    "            #raise\n",
    "            print('ERROR: ' + article_href)\n",
    "            continue\n",
    "        print( \"{:5d}: {}\".format(processed, article_href) )\n",
    "        if processed >= count:\n",
    "            break\n",
    "            \n",
    "    return article_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words( text, ignore_list ):\n",
    "    word_freq = defaultdict(int)\n",
    "    for word in word_tokenize(text):\n",
    "        word_lc = word.lower();\n",
    "        if word_lc not in ignore_list and word not in ignore_list:\n",
    "            word_freq[ word_lc ] += 1\n",
    "            #if word_freq[word_lc] > 2:\n",
    "                #print('       word at {:3d}: {}'.format(word_freq[word_lc], word_lc) )\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_frequencies( n, freq_list, max_cut, min_cut ):\n",
    "    freq_edit = dict( freq_list )\n",
    "    max_freq  = float( max(freq_list.values()) )\n",
    "    \n",
    "    for word in freq_list.keys():\n",
    "        word_freq = freq_list[word] / max_freq\n",
    "        if word_freq >= max_cut or word_freq <= min_cut:\n",
    "            del freq_edit[ word ]\n",
    "    #print( freq_edit )\n",
    "    return nlargest( n, freq_edit, key=freq_edit.get )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_links( article_links, count ):\n",
    "    unique_links = set([ re.sub('#(.*)$', '', href) for href in article_links ])\n",
    "    use_total    = min( count, unique_links.__len__() )\n",
    "    print( \"%d unique links found, trying to use using %d\" % (unique_links.__len__(), use_total) )\n",
    "    return list( unique_links )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soup_up( url ):\n",
    "    home_content  = urlopen( url ).read().decode('utf8')\n",
    "    home_soup     = BeautifulSoup( home_content, 'html.parser' )\n",
    "    return home_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlDailyMail( count, wordcount, ignore_list ):\n",
    "    home_soup     = soup_up('http://www.dailymail.co.uk/home/index.html');\n",
    "    article_links = [ a['href'] for a in home_soup.find_all('a', {'href': re.compile('/news/article-')}) ]\n",
    "    link_list     = process_links( article_links, count )\n",
    "    \n",
    "    def extract_content( link_prefix, article_href ):\n",
    "        article_soup = soup_up( link_prefix + article_href )\n",
    "        article_body = article_soup.find_all('div', {'itemprop':'articleBody'})\n",
    "        return [ a.find_all('p', {'class':'mol-para-with-font'}, recursive=False) for a in article_body ]\n",
    "    \n",
    "    return extract_articles(  'http://www.dailymail.co.uk', link_list, extract_content, 'daily mail', count )\n",
    "        \n",
    "print(\"Crawling the DM\")\n",
    "dm_vectors = crawlDailyMail( article_count, word_count, ignore_list )\n",
    "#print( dm_vectors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlGuardian( count, wordcount, ignore_list ):\n",
    "    home_soup     = soup_up('https://www.theguardian.com/uk-news');\n",
    "    article_links = [ a['href'] for a in home_soup.find_all('a', { 'href': re.compile('/\\d{4}/[a-z]{3}/\\d{1,2}/') }) ]\n",
    "    link_list     = process_links( article_links, count )\n",
    "    \n",
    "    link_list = list(map( lambda href: href.replace('https://www.theguardian.com', ''), link_list ))\n",
    "    def extract_content( link_prefix, article_href ):\n",
    "        article_soup = soup_up( link_prefix + article_href )\n",
    "        article_body = article_soup.find_all('div', {'itemprop':'articleBody'})\n",
    "        return [ a.find_all('p', recursive=False) for a in article_body ]\n",
    "    \n",
    "    return extract_articles( 'https://www.theguardian.com', link_list, extract_content, 'guardian', count )\n",
    "    \n",
    "print(\"Crawling the Guardian\")\n",
    "gd_vectors = crawlGuardian( article_count, word_count, ignore_list )\n",
    "#print( gd_vectors )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBayesNaive():\n",
    "    \n",
    "    def __init__( self, training_data ):\n",
    "        self._training_data = training_data\n",
    "        self._calculate_totals()\n",
    "        self._calculate_weights()\n",
    "        \n",
    "    def _calculate_totals( self ):\n",
    "        self._totals  = defaultdict(int)\n",
    "        self._classes = {}\n",
    "        \n",
    "        for vector in self._training_data:\n",
    "            if not vector['source'] in self._classes:\n",
    "                self._classes[ vector['source'] ] = defaultdict(int)\n",
    "            \n",
    "            for word in vector['words']:\n",
    "                self._totals[ word ] += 1\n",
    "                self._classes[ vector['source'] ][ word ] += 1\n",
    "                \n",
    "    def _calculate_weights( self ):\n",
    "        self._weights = {}\n",
    "        \n",
    "        for _class in self._classes.keys():\n",
    "            self._weights[ _class ] = { \\\n",
    "               word: float(i/self._totals[word]) \\\n",
    "               for word,i in self._classes[_class].items() }\n",
    "            \n",
    "    def test( self, text, word_count ):\n",
    "        test_freqs  = count_words( text, ignore_list )\n",
    "        test_vector = top_frequencies( word_count, test_freqs, 0.9, 0.1 )\n",
    "        \n",
    "        result = {}\n",
    "        for _class in self._classes.keys():\n",
    "            value = 1\n",
    "            for word in test_vector:\n",
    "                if word not in self._totals or word not in self._weights[ _class ]:\n",
    "                    continue;\n",
    "                value *= self._weights[ _class ][ word ]\n",
    "            result[ _class ] = value\n",
    "        \n",
    "        return {source: result[source] for source in sorted( result, key=result.get, reverse=True ) }\n",
    "        \n",
    "training_data = dm_vectors + gd_vectors\n",
    "bn = MyBayesNaive( training_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting BBC links...\n",
      "37 unique links found, trying to use using 3\n",
      "Testing /news/uk-42757023\n",
      "British tourists warned to 'stay in resorts' in Jamaica security emergency - BBC News\n",
      "guardian: 38.9151, daily mail: 0.7311\n",
      "\n",
      "Testing /news/entertainment-arts-42746617\n",
      "Pad Man: A man's 'period poverty' rescue becomes a film - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42712855\n",
      "Women's March: Where, when and why will protests happen? - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-europe-42749953\n",
      "France: Emmanuel Macron’s marvellous manoeuvres this week - BBC News\n",
      "daily mail: 27.0108, guardian: 7.7174\n",
      "\n",
      "Testing /news/world-europe-42750584\n",
      "Why Italians are saying 'No' to takeaway coffee - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42751715\n",
      "How can parents torture their children? - BBC News\n",
      "daily mail: 2.1439, guardian: 0.0988\n",
      "\n",
      "Testing /news/world-us-canada-39732845\n",
      "What has President Trump said about your country? - BBC News\n",
      "daily mail: 10000.0000, guardian: 10000.0000\n",
      "\n",
      "Testing /news/business-11428889\n",
      "Technology of Business - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42740239\n",
      "Why are these women sticking with Trump? - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/science-environment-42736930\n",
      "The man risking his life to save pink dolphins - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-middle-east-42704542\n",
      "Turkey targets Kurdish forces in Afrin: The short, medium and long story - BBC News\n",
      "daily mail: 259.9581, guardian: 153.7386\n",
      "\n",
      "Testing /news/world-us-canada-42759934\n",
      "US shutdown: Trump and Democrats blame each other - BBC News\n",
      "guardian: 43.5374, daily mail: 12.2449\n",
      "\n",
      "Testing /news/world-middle-east-42758532\n",
      "Syria conflict: 15 refugees found frozen to death - BBC News\n",
      "daily mail: 423.2804, guardian: 39.6825\n",
      "\n",
      "Testing /news/entertainment-arts-42746612\n",
      "Profoundly deaf Maisie Sly is the star of a short film which may be nominated for an Oscar - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-42753653\n",
      "In case you missed it - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/uk-england-sussex-42759124\n",
      "'Drunk' pilot removed from BA plane at Gatwick Airport - BBC News\n",
      "guardian: 896.6038, daily mail: 374.3396\n",
      "\n",
      "Testing /news/world-us-canada-42709360\n",
      "Trumplomacy: Has Trump made the world more dangerous? - BBC News\n",
      "daily mail: 9.7182, guardian: 3.6443\n",
      "\n",
      "Testing /news/world-asia-42551880\n",
      "North Korean skiers dream of Paralympic success - BBC News\n",
      "daily mail: 41.5195, guardian: 21.4852\n",
      "\n",
      "Testing /news/world-europe-42758188\n",
      "Turkey bus crash: Eleven killed in motorway accident - BBC News\n",
      "guardian: 404.2658, daily mail: 183.1009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the BBC!\n",
    "bbc_k = 5\n",
    "count = 3\n",
    "\n",
    "print('Getting BBC links...')\n",
    "bbc_soup  = soup_up('http://www.bbc.com/news')\n",
    "bbc_links = [ a['href'] for a in bbc_soup.find_all('a', { 'href': re.compile('^/news/(.*)\\d{6,}$') }) ]\n",
    "link_list = process_links( bbc_links, count )\n",
    "\n",
    "processed = 0;\n",
    "for test_link in link_list:\n",
    "    print('Testing ' + test_link )\n",
    "    test_soup = soup_up('http://www.bbc.com' + test_link)\n",
    "    print( test_soup.title.text )\n",
    "\n",
    "    test_body    = test_soup.find('div', {'property':'articleBody'})\n",
    "    if not test_body:\n",
    "        count += 1\n",
    "        print(' -- No test body found, continuing...\\n')\n",
    "        continue\n",
    "    test_para    = test_body.find_all('p', recursive=False )\n",
    "    if not test_para:\n",
    "        print(' -- No paragraphs found, continuing...\\n')\n",
    "        continue\n",
    "    \n",
    "    test_text   = ' '.join([ p.text for p in test_para ])\n",
    "    test_result = bn.test(test_text, word_count )\n",
    "\n",
    "    print(', '.join([\"{}: {:0.4f}\".format( source, score * 10000 ) for source, score in test_result.items() ]))\n",
    "    print('')\n",
    "    \n",
    "    processed += 1\n",
    "    if( processed >= count ):\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
