{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This may get a bit hefty for Notepad, but we'll see.\n",
    "# We'll control the counts from here:\n",
    "article_count = 50\n",
    "word_count    = 25\n",
    "num_k         = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ignore_list = set( stopwords.words('english')\n",
    "                  + list(punctuation) \n",
    "                  + ['’',\"'s\",\"'it\",\"'the\",\"‘\",\"'i\",\"n't\",'“','”','–','–','•','…','—'] \n",
    "                  + ['i','we','one','two','1','2','3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_articles( link_prefix, link_list, extract_function, source_name, count ):\n",
    "    processed = 0\n",
    "    article_words = []\n",
    "    for article_href in link_list:\n",
    "        try:\n",
    "            body_paragraphs = extract_function( link_prefix, article_href )\n",
    "            if not body_paragraphs:\n",
    "                raise ValueError('No body content found')\n",
    "            article_blocks  = []\n",
    "            for body in body_paragraphs:\n",
    "                article_blocks.append( \" \".join([ p.text.replace(u'\\xa0', '') for p in body ]) )\n",
    "            \n",
    "            article_text = \" \".join( article_blocks )\n",
    "            word_freq = count_words( article_text, ignore_list )\n",
    "            top_words = top_frequencies( word_count, word_freq, 0.9, 0.1 )\n",
    "            \n",
    "            article_words.append({ 'source':source_name, 'words': tuple(top_words), 'href': article_href })\n",
    "            processed += 1\n",
    "            \n",
    "        except:\n",
    "            #raise\n",
    "            print('ERROR: ' + article_href)\n",
    "            continue\n",
    "        print( \"{:5d}: {}\".format(processed, article_href) )\n",
    "        if processed >= count:\n",
    "            break\n",
    "            \n",
    "    return article_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words( text, ignore_list ):\n",
    "    word_freq = defaultdict(int)\n",
    "    for word in word_tokenize(text):\n",
    "        word_lc = word.lower();\n",
    "        if word_lc not in ignore_list and word not in ignore_list:\n",
    "            word_freq[ word_lc ] += 1\n",
    "            #if word_freq[word_lc] > 2:\n",
    "                #print('       word at {:3d}: {}'.format(word_freq[word_lc], word_lc) )\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_frequencies( n, freq_list, max_cut, min_cut ):\n",
    "    freq_edit = dict( freq_list )\n",
    "    max_freq  = float( max(freq_list.values()) )\n",
    "    \n",
    "    for word in freq_list.keys():\n",
    "        word_freq = freq_list[word] / max_freq\n",
    "        if word_freq >= max_cut or word_freq <= min_cut:\n",
    "            del freq_edit[ word ]\n",
    "    #print( freq_edit )\n",
    "    return nlargest( n, freq_edit, key=freq_edit.get )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_links( article_links, count ):\n",
    "    unique_links = set([ re.sub('#(.*)$', '', href) for href in article_links ])\n",
    "    use_total    = min( count, unique_links.__len__() )\n",
    "    print( \"%d unique links found, trying to use using %d\" % (unique_links.__len__(), use_total) )\n",
    "    return list( unique_links )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soup_up( url ):\n",
    "    home_content  = urlopen( url ).read().decode('utf8')\n",
    "    home_soup     = BeautifulSoup( home_content, 'html.parser' )\n",
    "    return home_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlDailyMail( count, wordcount, ignore_list ):\n",
    "    home_soup     = soup_up('http://www.dailymail.co.uk/home/index.html');\n",
    "    article_links = [ a['href'] for a in home_soup.find_all('a', {'href': re.compile('/news/article-')}) ]\n",
    "    link_list     = process_links( article_links, count )\n",
    "    \n",
    "    def extract_content( link_prefix, article_href ):\n",
    "        article_soup = soup_up( link_prefix + article_href )\n",
    "        article_body = article_soup.find_all('div', {'itemprop':'articleBody'})\n",
    "        return [ a.find_all('p', {'class':'mol-para-with-font'}, recursive=False) for a in article_body ]\n",
    "    \n",
    "    return extract_articles(  'http://www.dailymail.co.uk', link_list, extract_content, 'daily mail', count )\n",
    "        \n",
    "print(\"Crawling the DM\")\n",
    "dm_vectors = crawlDailyMail( article_count, word_count, ignore_list )\n",
    "#print( dm_vectors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlGuardian( count, wordcount, ignore_list ):\n",
    "    home_soup     = soup_up('https://www.theguardian.com/uk-news');\n",
    "    article_links = [ a['href'] for a in home_soup.find_all('a', { 'href': re.compile('/\\d{4}/[a-z]{3}/\\d{1,2}/') }) ]\n",
    "    link_list     = process_links( article_links, count )\n",
    "    \n",
    "    link_list = list(map( lambda href: href.replace('https://www.theguardian.com', ''), link_list ))\n",
    "    def extract_content( link_prefix, article_href ):\n",
    "        article_soup = soup_up( link_prefix + article_href )\n",
    "        article_body = article_soup.find_all('div', {'itemprop':'articleBody'})\n",
    "        return [ a.find_all('p', recursive=False) for a in article_body ]\n",
    "    \n",
    "    return extract_articles( 'https://www.theguardian.com', link_list, extract_content, 'guardian', count )\n",
    "    \n",
    "print(\"Crawling the Guardian\")\n",
    "gd_vectors = crawlGuardian( article_count, word_count, ignore_list )\n",
    "#print( gd_vectors )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, my version is based on possibly an oversimplification of the maths,\n",
    "# that we went through at the start of the course. Essentially, that for a \n",
    "# NB classififer, we first calculate the measure of each class for a word \n",
    "# which is characterised by countClass(word) / countTotal(word)\n",
    "# this was originally stated as Cspam(T) / Cspam(T) + Cham(t) (Spam Detection example)\n",
    "# I've (possibly wrongly) adjusted the denominator to be the total - isn't that effectively\n",
    "# what the sum of the two classes is?\n",
    "# Then, going through each word, the value of Cclass(word) is multiplied by the value.\n",
    "\n",
    "# So, having added in the MultinomialBayes, and seeing that it agrees with theirs, I'm\n",
    "# curious to know why mine isn't (quite) on the same page. IT definitely seems that I'm\n",
    "# missing something to do with the 'scaling', I wonder if that will sort it out.\n",
    "\n",
    "class MyBayesNaive():\n",
    "    \n",
    "    def __init__( self, training_data ):\n",
    "        self._training_data = training_data\n",
    "        self._calculate_totals()\n",
    "        self._calculate_weights()\n",
    "        \n",
    "    # So here we work out Cclass(word) and Ctotal(word)...\n",
    "    def _calculate_totals( self ):\n",
    "        self._word_totals     = defaultdict(int)\n",
    "        self._class_instances = defaultdict(int)\n",
    "        self._classes         = {}\n",
    "        \n",
    "        for vector in self._training_data:\n",
    "\n",
    "            self._class_instances[ vector['source'] ] += 1\n",
    "\n",
    "            if not vector['source'] in self._classes:\n",
    "                self._classes[ vector['source'] ] = defaultdict(int)\n",
    "            \n",
    "            for word in vector['words']:\n",
    "                self._word_totals[ word ] += 1\n",
    "                self._classes[ vector['source'] ][ word ] += 1\n",
    "                \n",
    "    # and then we do the division...\n",
    "    def _calculate_weights( self ):\n",
    "        self._word_weights = {}\n",
    "        \n",
    "        for _class in self._classes.keys():\n",
    "            # which happens here...\n",
    "            # i = number of times word appears in  _class\n",
    "            # CHANGE: from i/(total word instances) to i/(class instances in training data)\n",
    "            # because i only repesents a count of instances containing the word,\n",
    "            # this should work - we then need to scale in the test\n",
    "            self._word_weights[ _class ] = { \\\n",
    "               word: float(i/self._class_instances[ _class ]) \\\n",
    "                   for word,i in self._classes[_class].items() }\n",
    "            \n",
    "            \n",
    "    def test( self, text, word_count ):\n",
    "        test_freqs  = count_words( text, ignore_list )\n",
    "        test_vector = top_frequencies( word_count, test_freqs, 0.9, 0.1 )\n",
    "        \n",
    "        result = {}\n",
    "        for _class in self._classes.keys():\n",
    "            value = 1\n",
    "            for word in test_vector:\n",
    "                if word not in self._word_totals or word not in self._word_weights[ _class ]:\n",
    "                    value /= 1e3 # we'll steal this from them...\n",
    "                    continue;\n",
    "                value *= self._word_weights[ _class ][ word ]\n",
    "            result[ _class ] = value * float( self._class_instances[_class] / sum(self._class_instances.values()) )\n",
    "            \n",
    "        \n",
    "        return {source: result[source] for source in sorted( result, key=result.get, reverse=True ) }\n",
    "        \n",
    "training_data = dm_vectors + gd_vectors\n",
    "bn = MyBayesNaive( training_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, some of their maths is a little different to mine...\n",
    "# I'm not 100% correct it does what they say it does, but further investigation will\n",
    "# surely lead us to an answer.\n",
    "\n",
    "class TheirNaiveBayes():\n",
    "    \n",
    "    def __init__( self, training_data ):\n",
    "        self._sum_frequencies( training_data )\n",
    "    \n",
    "    def _sum_frequencies( self, training_data ):\n",
    "        self._frequencies = {}\n",
    "        for vector in training_data:\n",
    "            if vector['source'] not in self._frequencies:\n",
    "                self._frequencies[ vector['source'] ] = defaultdict(int)\n",
    "            \n",
    "            for word in vector['words']:\n",
    "                self._frequencies[ vector['source'] ][ word ] += 1\n",
    "                \n",
    "    def test( self, text, word_count ):\n",
    "        test_freqs  = count_words( text, ignore_list )\n",
    "        test_vector = top_frequencies( word_count, test_freqs, 0.9, 0.1 )\n",
    "        \n",
    "        class_results = defaultdict( lambda: 1 )\n",
    "        for word in test_vector:\n",
    "            for _class in self._frequencies.keys():\n",
    "                \n",
    "                if word in self._frequencies[ _class ]:\n",
    "                    # so this ratio seems to be different to mine.. they calcuate the value of a word \n",
    "                    # as being the Cclass(word) / Cclass(all words)\n",
    "                    # also, this feels computationally expensive - why sum it every time? It's not changing.\n",
    "                    class_results[ _class ] *= 1e3* self._frequencies[ _class ][ word ] \\\n",
    "                        / float( sum(self._frequencies[_class].values()) )\n",
    "                else:\n",
    "                    # This makes sense - if the word isn't there, don't just times by zero\n",
    "                    # that will instantly reset - I'm curious to know how they arrived at 1e3, though...\n",
    "                    # ( for comparison, if the word wasn't included, mine skips the multiply )\n",
    "                    class_results[ _class ] /= 1e3\n",
    "            \n",
    "        for _class in self._frequencies.keys():\n",
    "            # I guess this sort of makes sense - you scale the number against the total number of \n",
    "            # words counted in the category - at least, in theory that's what it does.\n",
    "            # see above, re: being computationally expensive.\n",
    "            class_results[ _class ] *= float( sum(self._frequencies[_class].values()) ) \\\n",
    "                / ( float(sum(self._frequencies[_class].values())) )\n",
    "                \n",
    "        return { s: class_results[s] for s in sorted(class_results, key=class_results.get, reverse=True) }\n",
    "                \n",
    "tbn = TheirNaiveBayes( training_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, let's get fancy and try it with a scikit class..\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "pred_array = [ {word: 1 for word in v['words']} for v in training_data ]\n",
    "targ_array = [ v['source']  for v in training_data ]\n",
    "vectors    = pd.DataFrame( pred_array, index=targ_array )\n",
    "vectors.fillna(0, inplace=True )\n",
    "\n",
    "columns = vectors.axes[1]\n",
    "classes = vectors.axes[0]\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit( vectors.values, classes )\n",
    "\n",
    "def testNaiveBayes( text, word_count ):\n",
    "    test_freqs  = count_words( text, ignore_list )\n",
    "    test_words  = top_frequencies( word_count, test_freqs, 0.9, 0.1 )\n",
    "    test_vector = [  1 if word in test_words else 0 for word in columns ]\n",
    "    return model.predict( [test_vector] )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting BBC links...\n",
      "40 unique links found, trying to use using 30\n",
      "Testing /news/entertainment-arts-42746617\n",
      "Pad Man: A man's 'period poverty' rescue becomes a film - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/uk-england-42765526\n",
      "Brownhills stabbing: Man arrested after girl, 8, killed - BBC News\n",
      "   My result: daily mail: 5.19e-53, guardian: 8.00e-57\n",
      "Their result: daily mail: 4.93e-39, guardian: 4.52e-46\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/world-europe-42750584\n",
      "Why Italians are saying 'No' to takeaway coffee - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-latin-america-42763471\n",
      "Brazil declares yellow fever emergency in Minas Gerais - BBC News\n",
      "   My result: guardian: 3.82e-56, daily mail: 2.76e-63\n",
      "Their result: guardian: 2.16e-45, daily mail: 5.78e-59\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/world-middle-east-42765697\n",
      "Syria: Turkey says ground troops moved in on Afrin - BBC News\n",
      "   My result: daily mail: 1.31e-59, guardian: 1.07e-59\n",
      "Their result: daily mail: 1.84e-50, guardian: 1.50e-50\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/entertainment-arts-42760180\n",
      "Kinks and Zombies bassist Jim Rodford dies aged 76 - BBC News\n",
      "   My result: daily mail: 4.38e-55, guardian: 9.29e-59\n",
      "Their result: daily mail: 2.51e-44, guardian: 3.27e-51\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/world-middle-east-42762501\n",
      "Why is Turkey attacking Syria? - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-asia-42763517\n",
      "Kabul: Afghan forces end Intercontinental Hotel siege - BBC News\n",
      "   My result: guardian: 4.63e-52, daily mail: 4.10e-54\n",
      "Their result: guardian: 4.19e-38, daily mail: 9.58e-42\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/business-11428889\n",
      "Technology of Business - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-europe-42725089\n",
      "Deported from Belgium, tortured in Sudan - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-middle-east-42704542\n",
      "Turkey targets Kurdish forces in Afrin: The short, medium and long story - BBC News\n",
      "   My result: daily mail: 2.53e-61, guardian: 9.67e-66\n",
      "Their result: daily mail: 2.16e-55, guardian: 1.33e-64\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/world-europe-42765092\n",
      "French President says Donald Trump is not a 'classical politician' - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/health-42705852\n",
      "When you can (and can't) eat carbs for dinner - BBC News\n",
      "   My result: guardian: 4.54e-64, daily mail: 3.06e-64\n",
      "Their result: guardian: 9.98e-60, daily mail: 6.42e-60\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/entertainment-arts-42746612\n",
      "Profoundly deaf Maisie Sly is the star of a short film which may be nominated for an Oscar - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-asia-42577024\n",
      "Why one man in Singapore was not allowed to adopt his child - BBC News\n",
      "   My result: daily mail: 5.23e-44, guardian: 2.05e-48\n",
      "Their result: daily mail: 5.54e-22, guardian: 2.97e-31\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/world-42736495\n",
      "Why do names matter so much? - BBC News\n",
      "   My result: guardian: 4.64e-62, daily mail: 8.17e-63\n",
      "Their result: guardian: 4.09e-56, daily mail: 6.96e-57\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/uk-scotland-42761139\n",
      "Travel warning as heavy snow hits Scotland - BBC News\n",
      "   My result: guardian: 3.38e-46, daily mail: 8.75e-51\n",
      "Their result: guardian: 3.15e-24, daily mail: 1.38e-33\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/world-africa-42748769\n",
      "Ellen Johnson Sirleaf: The legacy of Africa's first elected female president - BBC News\n",
      "   My result: guardian: 6.11e-47, daily mail: 1.40e-49\n",
      "Their result: guardian: 3.55e-28, daily mail: 5.41e-34\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/world-asia-42764971\n",
      "Kabul attack: Guests use sheets to escape hotel - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-latin-america-42761099\n",
      "The Brazilian man who lives in a sandcastle - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-asia-42765105\n",
      "North Korea Moranbong girl band leader heads Olympic inspection team - BBC News\n",
      "   My result: guardian: 7.92e-61, daily mail: 6.54e-62\n",
      "Their result: guardian: 2.79e-53, daily mail: 5.57e-56\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/world-latin-america-42763511\n",
      "Deadly violence over disputed Honduras election result - BBC News\n",
      "   My result: guardian: 4.11e-61, daily mail: 5.36e-63\n",
      "Their result: guardian: 1.45e-53, daily mail: 1.12e-58\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/world-africa-42766151\n",
      "DR Congo: Anti-Kabila protests dispersed with tear gas - BBC News\n",
      "   My result: guardian: 3.00e-56, daily mail: 2.85e-60\n",
      "Their result: guardian: 1.69e-45, daily mail: 2.43e-54\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/world-asia-china-42640569\n",
      "Sora Aoi: Japan's porn star who taught a Chinese generation about sex - BBC News\n",
      "   My result: guardian: 1.45e-52, daily mail: 3.65e-54\n",
      "Their result: guardian: 1.31e-38, daily mail: 8.52e-42\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/world-us-canada-42651688\n",
      "Trump's year on Twitter: Who has he criticised and praised the most? - BBC News\n",
      "   My result: guardian: 1.38e-53, daily mail: 1.18e-58\n",
      "Their result: guardian: 1.25e-39, daily mail: 4.10e-51\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/world-middle-east-29702440\n",
      "Who are the Kurds? - BBC News\n",
      "   My result: guardian: 2.54e-56, daily mail: 3.53e-57\n",
      "Their result: guardian: 1.44e-45, daily mail: 2.03e-46\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/business-12686570\n",
      "Global education - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/business-22434141\n",
      "Entrepreneurship - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/stories-42665317\n",
      "I advertised for a man to get me pregnant - then I fell in love - BBC News\n",
      "   My result: daily mail: 1.98e-24, guardian: 4.36e-26\n",
      "Their result: daily mail: 4.62e-12, guardian: 2.46e-15\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/world-africa-42761450\n",
      "Football president George Weah puts Liberian army to the test - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-latin-america-42763291\n",
      "Pope in Peru: Francis speaks out on violence against women - BBC News\n",
      "   My result: guardian: 3.63e-55, daily mail: 2.03e-59\n",
      "Their result: guardian: 8.22e-43, daily mail: 7.03e-52\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/world-us-canada-42762505\n",
      "Women's marches across the US - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-asia-42729173\n",
      "New Zealand debates access to dead sea life footage - BBC News\n",
      "   My result: guardian: 4.91e-56, daily mail: 1.63e-59\n",
      "Their result: guardian: 2.77e-45, daily mail: 5.67e-52\n",
      "Naive result: guardian\n",
      "\n",
      "Testing /news/uk-scotland-highlands-islands-42765306\n",
      "Climbers rescued from snow-covered ridge - BBC News\n",
      "   My result: daily mail: 1.23e-56, guardian: 9.85e-60\n",
      "Their result: daily mail: 2.86e-44, guardian: 3.47e-52\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/stories-42708922\n",
      "'He wasn't happy until he had me all to himself' - BBC News\n",
      "   My result: daily mail: 1.09e-37, guardian: 4.76e-46\n",
      "Their result: daily mail: 3.16e-09, guardian: 1.11e-25\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/world-europe-42707957\n",
      "Putin cast as national saviour ahead of Russia election - BBC News\n",
      "   My result: guardian: 9.60e-59, daily mail: 2.57e-59\n",
      "Their result: guardian: 1.35e-49, daily mail: 8.93e-52\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/world-42639877\n",
      "Edible insects: Do insects actually taste any good? - BBC News\n",
      "   My result: daily mail: 1.47e-59, guardian: 1.03e-62\n",
      "Their result: daily mail: 2.07e-50, guardian: 9.08e-57\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/world-42763620\n",
      "Would you swap your pet for a snail? - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-asia-india-42637998\n",
      "Dark is divine: What colour are Indian gods and goddesses? - BBC News\n",
      "   My result: daily mail: 5.15e-58, guardian: 1.35e-60\n",
      "Their result: daily mail: 7.26e-49, guardian: 1.19e-54\n",
      "Naive result: daily mail\n",
      "\n",
      "Testing /news/world-us-canada-42765101\n",
      "US shutdown: Senate in bid to end impasse - BBC News\n",
      "   My result: daily mail: 3.15e-46, guardian: 2.70e-48\n",
      "Their result: daily mail: 2.02e-27, guardian: 9.80e-33\n",
      "Naive result: daily mail\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the BBC!\n",
    "count = 30\n",
    "\n",
    "print('Getting BBC links...')\n",
    "bbc_soup  = soup_up('http://www.bbc.com/news')\n",
    "bbc_links = [ a['href'] for a in bbc_soup.find_all('a', { 'href': re.compile('^/news/(.*)\\d{6,}$') }) ]\n",
    "link_list = process_links( bbc_links, count )\n",
    "\n",
    "processed = 0;\n",
    "for test_link in link_list:\n",
    "    print('Testing ' + test_link )\n",
    "    test_soup = soup_up('http://www.bbc.com' + test_link)\n",
    "    print( test_soup.title.text )\n",
    "\n",
    "    test_body    = test_soup.find('div', {'property':'articleBody'})\n",
    "    if not test_body:\n",
    "        count += 1\n",
    "        print(' -- No test body found, continuing...\\n')\n",
    "        continue\n",
    "    test_para    = test_body.find_all('p', recursive=False )\n",
    "    if not test_para:\n",
    "        print(' -- No paragraphs found, continuing...\\n')\n",
    "        continue\n",
    "    \n",
    "    test_text   = ' '.join([ p.text for p in test_para ])\n",
    "    test_result = bn.test(test_text, word_count )\n",
    "    nvby_result = testNaiveBayes( test_text, word_count )\n",
    "\n",
    "    print(\"   My result: \" + ', '.join([\"{}: {:2.2e}\" \\\n",
    "       .format( source, score * 10000 ) for source, score in test_result.items() ]))\n",
    "    \n",
    "    their_result = tbn.test( test_text, word_count )\n",
    "    print( \"Their result: \" + ', '.join([\"{}: {:2.2e}\" \\\n",
    "        .format( source, score ) for source, score in their_result.items() ]))\n",
    "    \n",
    "    print(\"Naive result: \" + nvby_result[0] )\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    processed += 1\n",
    "    if( processed >= count ):\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
