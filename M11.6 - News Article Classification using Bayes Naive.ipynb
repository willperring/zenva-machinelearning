{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This may get a bit hefty for Notepad, but we'll see.\n",
    "# We'll control the counts from here:\n",
    "article_count = 50\n",
    "word_count    = 25\n",
    "num_k         = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ignore_list = set( stopwords.words('english')\n",
    "                  + list(punctuation) \n",
    "                  + ['’',\"'s\",\"'it\",\"'the\",\"‘\",\"'i\",\"n't\",'“','”','–','–','•','…','—'] \n",
    "                  + ['i','we','one','two','1','2','3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_articles( link_prefix, link_list, extract_function, source_name, count ):\n",
    "    processed = 0\n",
    "    article_words = []\n",
    "    for article_href in link_list:\n",
    "        try:\n",
    "            body_paragraphs = extract_function( link_prefix, article_href )\n",
    "            if not body_paragraphs:\n",
    "                raise ValueError('No body content found')\n",
    "            article_blocks  = []\n",
    "            for body in body_paragraphs:\n",
    "                article_blocks.append( \" \".join([ p.text.replace(u'\\xa0', '') for p in body ]) )\n",
    "            \n",
    "            article_text = \" \".join( article_blocks )\n",
    "            word_freq = count_words( article_text, ignore_list )\n",
    "            top_words = top_frequencies( word_count, word_freq, 0.9, 0.1 )\n",
    "            \n",
    "            article_words.append({ 'source':source_name, 'words': tuple(top_words), 'href': article_href })\n",
    "            processed += 1\n",
    "            \n",
    "        except:\n",
    "            #raise\n",
    "            print('ERROR: ' + article_href)\n",
    "            continue\n",
    "        print( \"{:5d}: {}\".format(processed, article_href) )\n",
    "        if processed >= count:\n",
    "            break\n",
    "            \n",
    "    return article_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words( text, ignore_list ):\n",
    "    word_freq = defaultdict(int)\n",
    "    for word in word_tokenize(text):\n",
    "        word_lc = word.lower();\n",
    "        if word_lc not in ignore_list and word not in ignore_list:\n",
    "            word_freq[ word_lc ] += 1\n",
    "            #if word_freq[word_lc] > 2:\n",
    "                #print('       word at {:3d}: {}'.format(word_freq[word_lc], word_lc) )\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_frequencies( n, freq_list, max_cut, min_cut ):\n",
    "    freq_edit = dict( freq_list )\n",
    "    max_freq  = float( max(freq_list.values()) )\n",
    "    \n",
    "    for word in freq_list.keys():\n",
    "        word_freq = freq_list[word] / max_freq\n",
    "        if word_freq >= max_cut or word_freq <= min_cut:\n",
    "            del freq_edit[ word ]\n",
    "    #print( freq_edit )\n",
    "    return nlargest( n, freq_edit, key=freq_edit.get )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_links( article_links, count ):\n",
    "    unique_links = set([ re.sub('#(.*)$', '', href) for href in article_links ])\n",
    "    use_total    = min( count, unique_links.__len__() )\n",
    "    print( \"%d unique links found, trying to use using %d\" % (unique_links.__len__(), use_total) )\n",
    "    return list( unique_links )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soup_up( url ):\n",
    "    home_content  = urlopen( url ).read().decode('utf8')\n",
    "    home_soup     = BeautifulSoup( home_content, 'html.parser' )\n",
    "    return home_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlDailyMail( count, wordcount, ignore_list ):\n",
    "    home_soup     = soup_up('http://www.dailymail.co.uk/home/index.html');\n",
    "    article_links = [ a['href'] for a in home_soup.find_all('a', {'href': re.compile('/news/article-')}) ]\n",
    "    link_list     = process_links( article_links, count )\n",
    "    \n",
    "    def extract_content( link_prefix, article_href ):\n",
    "        article_soup = soup_up( link_prefix + article_href )\n",
    "        article_body = article_soup.find_all('div', {'itemprop':'articleBody'})\n",
    "        return [ a.find_all('p', {'class':'mol-para-with-font'}, recursive=False) for a in article_body ]\n",
    "    \n",
    "    return extract_articles(  'http://www.dailymail.co.uk', link_list, extract_content, 'daily mail', count )\n",
    "        \n",
    "print(\"Crawling the DM\")\n",
    "dm_vectors = crawlDailyMail( article_count, word_count, ignore_list )\n",
    "#print( dm_vectors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlGuardian( count, wordcount, ignore_list ):\n",
    "    home_soup     = soup_up('https://www.theguardian.com/uk-news');\n",
    "    article_links = [ a['href'] for a in home_soup.find_all('a', { 'href': re.compile('/\\d{4}/[a-z]{3}/\\d{1,2}/') }) ]\n",
    "    link_list     = process_links( article_links, count )\n",
    "    \n",
    "    link_list = list(map( lambda href: href.replace('https://www.theguardian.com', ''), link_list ))\n",
    "    def extract_content( link_prefix, article_href ):\n",
    "        article_soup = soup_up( link_prefix + article_href )\n",
    "        article_body = article_soup.find_all('div', {'itemprop':'articleBody'})\n",
    "        return [ a.find_all('p', recursive=False) for a in article_body ]\n",
    "    \n",
    "    return extract_articles( 'https://www.theguardian.com', link_list, extract_content, 'guardian', count )\n",
    "    \n",
    "print(\"Crawling the Guardian\")\n",
    "gd_vectors = crawlGuardian( article_count, word_count, ignore_list )\n",
    "#print( gd_vectors )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, my version is based on possibly an oversimplification of the maths,\n",
    "# that we went through at the start of the course. Essentially, that for a \n",
    "# NB classififer, we first calculate the measure of each class for a word \n",
    "# which is characterised by countClass(word) / countTotal(word)\n",
    "# this was originally stated as Cspam(T) / Cspam(T) + Cham(t) (Spam Detection example)\n",
    "# I've (possibly wrongly) adjusted the denominator to be the total - isn't that effectively\n",
    "# what the sum of the two classes is?\n",
    "# Then, going through each word, the value of Cclass(word) is multiplied by the value.\n",
    "\n",
    "class MyBayesNaive():\n",
    "    \n",
    "    def __init__( self, training_data ):\n",
    "        self._training_data = training_data\n",
    "        self._calculate_totals()\n",
    "        self._calculate_weights()\n",
    "        \n",
    "    # So here we work out Cclass(word) and Ctotal(word)...\n",
    "    def _calculate_totals( self ):\n",
    "        self._totals  = defaultdict(int)\n",
    "        self._classes = {}\n",
    "        \n",
    "        for vector in self._training_data:\n",
    "            if not vector['source'] in self._classes:\n",
    "                self._classes[ vector['source'] ] = defaultdict(int)\n",
    "            \n",
    "            for word in vector['words']:\n",
    "                self._totals[ word ] += 1\n",
    "                self._classes[ vector['source'] ][ word ] += 1\n",
    "                \n",
    "    # and then we do the division...\n",
    "    def _calculate_weights( self ):\n",
    "        self._weights = {}\n",
    "        \n",
    "        for _class in self._classes.keys():\n",
    "            # which happens here...\n",
    "            self._weights[ _class ] = { \\\n",
    "               word: float(i/self._totals[word]) \\\n",
    "                   for word,i in self._classes[_class].items() }\n",
    "            \n",
    "    def test( self, text, word_count ):\n",
    "        test_freqs  = count_words( text, ignore_list )\n",
    "        test_vector = top_frequencies( word_count, test_freqs, 0.9, 0.1 )\n",
    "        \n",
    "        result = {}\n",
    "        for _class in self._classes.keys():\n",
    "            value = 1\n",
    "            for word in test_vector:\n",
    "                if word not in self._totals or word not in self._weights[ _class ]:\n",
    "                    continue;\n",
    "                value *= self._weights[ _class ][ word ]\n",
    "            result[ _class ] = value\n",
    "        \n",
    "        return {source: result[source] for source in sorted( result, key=result.get, reverse=True ) }\n",
    "        \n",
    "training_data = dm_vectors + gd_vectors\n",
    "bn = MyBayesNaive( training_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, some of their maths is a little different to mine...\n",
    "# I'm not 100% correct it does what they say it does, but further investigation will\n",
    "# surely lead us to an answer.\n",
    "\n",
    "class TheirNaiveBayes():\n",
    "    \n",
    "    def __init__( self, training_data ):\n",
    "        self._sum_frequencies( training_data )\n",
    "    \n",
    "    def _sum_frequencies( self, training_data ):\n",
    "        self._frequencies = {}\n",
    "        for vector in training_data:\n",
    "            if vector['source'] not in self._frequencies:\n",
    "                self._frequencies[ vector['source'] ] = defaultdict(int)\n",
    "            \n",
    "            for word in vector['words']:\n",
    "                self._frequencies[ vector['source'] ][ word ] += 1\n",
    "                \n",
    "    def test( self, text, word_count ):\n",
    "        test_freqs  = count_words( text, ignore_list )\n",
    "        test_vector = top_frequencies( word_count, test_freqs, 0.9, 0.1 )\n",
    "        \n",
    "        class_results = defaultdict( lambda: 1 )\n",
    "        for word in test_vector:\n",
    "            for _class in self._frequencies.keys():\n",
    "                \n",
    "                if word in self._frequencies[ _class ]:\n",
    "                    # so this ratio seems to be different to mine.. they calcuate the value of a word \n",
    "                    # as being the Cclass(word) / Cclass(all words)\n",
    "                    class_results[ _class ] *= 1e3* self._frequencies[ _class ][ word ] \\\n",
    "                        / float( sum(self._frequencies[_class].values()) )\n",
    "                else:\n",
    "                    # This makes sense - if the word isn't there, don't just times by zero\n",
    "                    # that will instantly reset - I'm curious to know how they arrived at 1e3, though...\n",
    "                    # ( for comparison, if the word wasn't included, mine skips the multiply )\n",
    "                    class_results[ _class ] /= 1e3\n",
    "            \n",
    "        for _class in self._frequencies.keys():\n",
    "            # I guess this sort of makes sense - you scale the number against the total number of \n",
    "            # words counted in the category - at least, in theory that's what it does.\n",
    "            class_results[ _class ] *= float( sum(self._frequencies[_class].values()) ) \\\n",
    "                / ( float(sum(self._frequencies[_class].values())) )\n",
    "                \n",
    "        return { s: class_results[s] for s in sorted(class_results, key=class_results.get, reverse=True) }\n",
    "                \n",
    "tbn = TheirNaiveBayes( training_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting BBC links...\n",
      "36 unique links found, trying to use using 3\n",
      "Testing /news/uk-42757023\n",
      "British tourists warned to 'stay in resorts' in Jamaica security emergency - BBC News\n",
      "   My result: guardian: 38.9151, daily mail: 0.7311\n",
      "Their result: guardian: 2.371600485867787e-13, daily mail: 1.703439100604591e-42\n",
      "\n",
      "Testing /news/entertainment-arts-42746617\n",
      "Pad Man: A man's 'period poverty' rescue becomes a film - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42712855\n",
      "Women's March: Where, when and why will protests happen? - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-europe-42749953\n",
      "France: Emmanuel Macron’s marvellous manoeuvres this week - BBC News\n",
      "   My result: daily mail: 27.0108, guardian: 7.7174\n",
      "Their result: daily mail: 7.130454673630771e-34, guardian: 2.2265447100480167e-34\n",
      "\n",
      "Testing /news/world-europe-42750584\n",
      "Why Italians are saying 'No' to takeaway coffee - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42751715\n",
      "How can parents torture their children? - BBC News\n",
      "   My result: daily mail: 2.1439, guardian: 0.0988\n",
      "Their result: daily mail: 1.3949620571505283e-21, guardian: 8.454129179105933e-25\n",
      "\n",
      "Testing /news/world-us-canada-39732845\n",
      "What has President Trump said about your country? - BBC News\n",
      "   My result: daily mail: 10000.0000, guardian: 10000.0000\n",
      "Their result: daily mail: 1.0, guardian: 1.0\n",
      "\n",
      "Testing /news/business-11428889\n",
      "Technology of Business - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42740239\n",
      "Why are these women sticking with Trump? - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/science-environment-42736930\n",
      "The man risking his life to save pink dolphins - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-middle-east-42704542\n",
      "Turkey targets Kurdish forces in Afrin: The short, medium and long story - BBC News\n",
      "   My result: daily mail: 2339.6226, guardian: 553.4591\n",
      "Their result: daily mail: 2.1590447215766845e-55, guardian: 1.3259269275085809e-64\n",
      "\n",
      "Testing /news/world-us-canada-42759934\n",
      "US shutdown: Trump and Democrats blame each other - BBC News\n",
      "   My result: guardian: 43.5374, daily mail: 12.2449\n",
      "Their result: guardian: 3.5856305697876056e-41, daily mail: 1.64848945219799e-42\n",
      "\n",
      "Testing /news/world-middle-east-42758532\n",
      "Syria conflict: 15 refugees found frozen to death - BBC News\n",
      "   My result: daily mail: 423.2804, guardian: 39.6825\n",
      "Their result: daily mail: 3.688808972840573e-50, guardian: 6.197226925018521e-54\n",
      "\n",
      "Testing /news/world-42753653\n",
      "In case you missed it - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/uk-england-sussex-42759124\n",
      "'Drunk' pilot removed from BA plane at Gatwick Airport - BBC News\n",
      "   My result: guardian: 896.6038, daily mail: 374.3396\n",
      "Their result: daily mail: 7.753802424603861e-47, guardian: 4.0718866063161624e-61\n",
      "\n",
      "Testing /news/world-us-canada-42709360\n",
      "Trumplomacy: Has Trump made the world more dangerous? - BBC News\n",
      "   My result: daily mail: 9.7182, guardian: 3.6443\n",
      "Their result: guardian: 6.989194096483076e-28, daily mail: 3.233766291896043e-37\n",
      "\n",
      "Testing /news/world-europe-42758188\n",
      "Turkey bus crash: Eleven killed in motorway accident - BBC News\n",
      "   My result: guardian: 404.2658, daily mail: 183.1009\n",
      "Their result: daily mail: 3.216180323195371e-49, guardian: 2.0977117363033685e-54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the BBC!\n",
    "bbc_k = 5\n",
    "count = 3\n",
    "\n",
    "print('Getting BBC links...')\n",
    "bbc_soup  = soup_up('http://www.bbc.com/news')\n",
    "bbc_links = [ a['href'] for a in bbc_soup.find_all('a', { 'href': re.compile('^/news/(.*)\\d{6,}$') }) ]\n",
    "link_list = process_links( bbc_links, count )\n",
    "\n",
    "processed = 0;\n",
    "for test_link in link_list:\n",
    "    print('Testing ' + test_link )\n",
    "    test_soup = soup_up('http://www.bbc.com' + test_link)\n",
    "    print( test_soup.title.text )\n",
    "\n",
    "    test_body    = test_soup.find('div', {'property':'articleBody'})\n",
    "    if not test_body:\n",
    "        count += 1\n",
    "        print(' -- No test body found, continuing...\\n')\n",
    "        continue\n",
    "    test_para    = test_body.find_all('p', recursive=False )\n",
    "    if not test_para:\n",
    "        print(' -- No paragraphs found, continuing...\\n')\n",
    "        continue\n",
    "    \n",
    "    test_text   = ' '.join([ p.text for p in test_para ])\n",
    "    test_result = bn.test(test_text, word_count )\n",
    "\n",
    "    print(\"   My result: \" + ', '.join([\"{}: {:0.4f}\" \\\n",
    "       .format( source, score * 10000 ) for source, score in test_result.items() ]))\n",
    "    \n",
    "    their_result = tbn.test( test_text, word_count )\n",
    "    print( \"Their result: \" + ', '.join([\"{}: {}\" \\\n",
    "        .format( source, str(score) ) for source, score in their_result.items() ]))\n",
    "    \n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    processed += 1\n",
    "    if( processed >= count ):\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
