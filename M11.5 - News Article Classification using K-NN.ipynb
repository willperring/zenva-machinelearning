{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So, we want to be able to classify articles into tech and non-tech using K-NN.\n",
    "# How will we go about this? (I think I might go for Daily Mail / Guardian instead)\n",
    "\n",
    "# Additional: If anyone else is reading this, apologies - it could be neater. I wasn't\n",
    "# expecting to have quite so much fun and go so far with it, hence why it's now 2:20am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll attempt the following steps:\n",
    "# 1. Download a relevant corpus - pick a new website and extract two sets of article: tech and sport.\n",
    "# 2. Represent each article as a vector of the 25 most important words in an article.\n",
    "# 3. The distance between articles is calculated using the number of words that they have in common\n",
    "# 4. Find the K-Nearest Neighbours and carry out a majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This may get a bit hefty for Notepad, but we'll see.\n",
    "# We'll control the counts from here:\n",
    "article_count = 50\n",
    "word_count    = 25\n",
    "num_k         = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ignore_list = set( stopwords.words('english')\n",
    "                  + list(punctuation) \n",
    "                  + ['’',\"'s\",\"'it\",\"'the\",\"‘\",\"'i\",\"n't\",'“','”','–','–','•','…','—'] \n",
    "                  + ['i','we','one','two','1','2','3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_articles( link_prefix, link_list, extract_function, source_name, count ):\n",
    "    processed = 0\n",
    "    article_words = []\n",
    "    for article_href in link_list:\n",
    "        try:\n",
    "            body_paragraphs = extract_function( link_prefix, article_href )\n",
    "            if not body_paragraphs:\n",
    "                raise ValueError('No body content found')\n",
    "            article_blocks  = []\n",
    "            for body in body_paragraphs:\n",
    "                article_blocks.append( \" \".join([ p.text.replace(u'\\xa0', '') for p in body ]) )\n",
    "            \n",
    "            article_text = \" \".join( article_blocks )\n",
    "            word_freq = count_words( article_text, ignore_list )\n",
    "            top_words = top_frequencies( word_count, word_freq, 0.9, 0.1 )\n",
    "            \n",
    "            article_words.append({ 'source':source_name, 'words': tuple(top_words), 'href': article_href })\n",
    "            processed += 1\n",
    "            \n",
    "        except:\n",
    "            #raise\n",
    "            print('ERROR: ' + article_href)\n",
    "            continue\n",
    "        print( \"{:5d}: {}\".format(processed, article_href) )\n",
    "        if processed >= count:\n",
    "            break\n",
    "            \n",
    "    return article_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words( text, ignore_list ):\n",
    "    word_freq = defaultdict(int)\n",
    "    for word in word_tokenize(text):\n",
    "        word_lc = word.lower();\n",
    "        if word_lc not in ignore_list and word not in ignore_list:\n",
    "            word_freq[ word_lc ] += 1\n",
    "            #if word_freq[word_lc] > 2:\n",
    "                #print('       word at {:3d}: {}'.format(word_freq[word_lc], word_lc) )\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_frequencies( n, freq_list, max_cut, min_cut ):\n",
    "    freq_edit = dict( freq_list )\n",
    "    max_freq  = float( max(freq_list.values()) )\n",
    "    \n",
    "    for word in freq_list.keys():\n",
    "        word_freq = freq_list[word] / max_freq\n",
    "        if word_freq >= max_cut or word_freq <= min_cut:\n",
    "            del freq_edit[ word ]\n",
    "    #print( freq_edit )\n",
    "    return nlargest( n, freq_edit, key=freq_edit.get )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_links( article_links, count ):\n",
    "    unique_links = set([ re.sub('#(.*)$', '', href) for href in article_links ])\n",
    "    use_total    = min( count, unique_links.__len__() )\n",
    "    print( \"%d unique links found, trying to use using %d\" % (unique_links.__len__(), use_total) )\n",
    "    return list( unique_links )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soup_up( url ):\n",
    "    home_content  = urlopen( url ).read().decode('utf8')\n",
    "    home_soup     = BeautifulSoup( home_content, 'html.parser' )\n",
    "    return home_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlDailyMail( count, wordcount, ignore_list ):\n",
    "    home_soup     = soup_up('http://www.dailymail.co.uk/home/index.html');\n",
    "    article_links = [ a['href'] for a in home_soup.find_all('a', {'href': re.compile('/news/article-')}) ]\n",
    "    link_list     = process_links( article_links, count )\n",
    "    \n",
    "    def extract_content( link_prefix, article_href ):\n",
    "        article_soup = soup_up( link_prefix + article_href )\n",
    "        article_body = article_soup.find_all('div', {'itemprop':'articleBody'})\n",
    "        return [ a.find_all('p', {'class':'mol-para-with-font'}, recursive=False) for a in article_body ]\n",
    "    \n",
    "    return extract_articles(  'http://www.dailymail.co.uk', link_list, extract_content, 'daily mail', count )\n",
    "        \n",
    "print(\"Crawling the DM\")\n",
    "dm_vectors = crawlDailyMail( article_count, word_count, ignore_list )\n",
    "#print( dm_vectors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlGuardian( count, wordcount, ignore_list ):\n",
    "    home_soup     = soup_up('https://www.theguardian.com/uk-news');\n",
    "    article_links = [ a['href'] for a in home_soup.find_all('a', { 'href': re.compile('/\\d{4}/[a-z]{3}/\\d{1,2}/') }) ]\n",
    "    link_list     = process_links( article_links, count )\n",
    "    \n",
    "    link_list = list(map( lambda href: href.replace('https://www.theguardian.com', ''), link_list ))\n",
    "    def extract_content( link_prefix, article_href ):\n",
    "        article_soup = soup_up( link_prefix + article_href )\n",
    "        article_body = article_soup.find_all('div', {'itemprop':'articleBody'})\n",
    "        return [ a.find_all('p', recursive=False) for a in article_body ]\n",
    "    \n",
    "    return extract_articles( 'https://www.theguardian.com', link_list, extract_content, 'guardian', count )\n",
    "    \n",
    "print(\"Crawling the Guardian\")\n",
    "gd_vectors = crawlGuardian( article_count, word_count, ignore_list )\n",
    "#print( gd_vectors )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlIndependent( count, wordcount, ignorelist ):\n",
    "    home_soup     = soup_up('http://www.independent.co.uk/');\n",
    "    article_links = [ a['href'] for a in home_soup.find_all('a', { 'href': re.compile('^/news/(.*)\\d{5,}.html') }) ]\n",
    "    link_list     = process_links( article_links, count )\n",
    "   \n",
    "    def extract_content( link_prefix, article_href ):\n",
    "        article_soup = soup_up( link_prefix + article_href )\n",
    "        article_body = article_soup.find_all('div', {'itemprop':'articleBody'})\n",
    "        return [ a.find_all('p', recursive=False) for a in article_body ]\n",
    "    \n",
    "    return extract_articles(  'http://www.independent.co.uk', link_list, extract_content, 'independent', count )\n",
    "\n",
    "print('Crawling The Independent')\n",
    "id_vectors = crawlIndependent( article_count, word_count, ignore_list )\n",
    "#print( id_vectors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlExpress( count, wordcount, ignorelist ):\n",
    "    home_soup     = soup_up('https://www.express.co.uk/news' );\n",
    "    article_links = [ a['href'] for a in home_soup.find_all('a', { 'href': re.compile('^/news/[a-z]*/\\d*/') }) ]\n",
    "    link_list     = process_links( article_links, count )\n",
    "    \n",
    "    def extract_content( link_prefix, article_href ):\n",
    "        article_soup = soup_up( link_prefix + article_href )\n",
    "        article_body = article_soup.find_all('section', {'class':'text-description'})\n",
    "        return [ a.find_all('p', recursive=False) for a in article_body ]\n",
    "    \n",
    "    return extract_articles( 'http://www.express.co.uk', link_list, extract_content, 'express', count )\n",
    "\n",
    "print('Crawling The Express')\n",
    "ex_vectors = crawlExpress( article_count, word_count, ignore_list )\n",
    "#print( ex_vectors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlTelegraph( count, wordcount, ignorelist ):\n",
    "    home_soup     = soup_up('http://www.telegraph.co.uk/news/');\n",
    "    article_links = [ a['href'] for a in home_soup.find_all('a', { 'href': re.compile('^/news/\\d{4}/') }) ]\n",
    "    link_list     = process_links( article_links, count )\n",
    "\n",
    "    def extract_content( link_prefix, article_href ):\n",
    "        article_soup = soup_up( link_prefix + article_href )\n",
    "        premium_wall = article_soup.find('div', {'class': 'premium-paywall'})\n",
    "        if premium_wall:\n",
    "            raise Exception('Premium Article')\n",
    "        article_body = article_soup.find_all('div', {'class':'article-body-text'})\n",
    "        article_para = [ body.find_all('p') for body in article_body ]\n",
    "        return article_para\n",
    "\n",
    "    return extract_articles( 'http://www.telegraph.co.uk', link_list, extract_content, 'telegraph', count )\n",
    "\n",
    "print('Crawling The Telegraph')\n",
    "tg_vectors = crawlTelegraph( article_count, word_count, ignore_list )\n",
    "#print( tg_vectors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSpace:\n",
    "    \n",
    "    def __init__( self, training_data, ignore_list ):\n",
    "        self._training_data = training_data\n",
    "        self._ignore_list   = ignore_list\n",
    "    \n",
    "    def _get_class_pc( self ):\n",
    "        pass\n",
    "    \n",
    "    def test( self, text, wordcount, k ):\n",
    "        neighbours = defaultdict(int)\n",
    "        word_freq  = count_words( text, self._ignore_list )\n",
    "        test_set   = set( word_freq )\n",
    "        \n",
    "        for article in self._training_data:\n",
    "            neighbours[ (article['source'], article['href']) ] = len( set(article['words']).intersection(test_set) )\n",
    "        \n",
    "        k_nearest       = nlargest( k, neighbours, key=neighbours.get )\n",
    "        classifications = defaultdict(int)\n",
    "        for neighbour in k_nearest:\n",
    "            classifications[ neighbour[0] ] += 1\n",
    "        \n",
    "        return [ (k, classifications[k]) for k in sorted(classifications, key=classifications.get, reverse=True) ]\n",
    "        \n",
    "training_data = dm_vectors + gd_vectors + id_vectors + ex_vectors + tg_vectors\n",
    "vs = VectorSpace( training_data, ignore_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting BBC links...\n",
      "42 unique links found, trying to use using 25\n",
      "Testing /news/business-22434141\n",
      "Entrepreneurship - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/science-environment-42747272\n",
      "Nasa removes US astronaut from ISS mission - BBC News\n",
      "40% independent, 40% guardian, 20% daily mail\n",
      "\n",
      "Testing /news/world-us-canada-42653793\n",
      "Ten ways Trump has changed America - BBC News\n",
      "60% independent, 20% express, 20% telegraph\n",
      "\n",
      "Testing /news/world-us-canada-42754607\n",
      "Nassar case: Gold medallists Raisman and Wieber face abuser - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-asia-42758184\n",
      "Asian wildlife trafficking 'kingpin' Boonchai Bach arrested - BBC News\n",
      "60% telegraph, 20% independent, 20% guardian\n",
      "\n",
      "Testing /news/entertainment-arts-42746612\n",
      "Profoundly deaf Maisie Sly is the star of a short film which may be nominated for an Oscar - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/entertainment-arts-42746617\n",
      "Pad Man: A man's 'period poverty' rescue becomes a film - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42548824\n",
      "Lac-Megantic: The runaway train that destroyed a town - BBC News\n",
      "40% daily mail, 40% independent, 20% guardian\n",
      "\n",
      "Testing /news/science-environment-42736930\n",
      "The man risking his life to save pink dolphins - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42738881\n",
      "What is Trumpism? - BBC News\n",
      "40% guardian, 20% independent, 20% daily mail, 20% telegraph\n",
      "\n",
      "Testing /news/technology-42730916\n",
      "Virgin's Hyperloop: Future or fantasy? - BBC News\n",
      "40% independent, 40% daily mail, 20% express\n",
      "\n",
      "Testing /news/technology-42742168\n",
      "The future of high-speed travel? - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-39732845\n",
      "What has President Trump said about your country? - BBC News\n",
      "40% guardian, 40% express, 20% independent\n",
      "\n",
      "Testing /news/business-12686570\n",
      "Global education - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-europe-42749953\n",
      "France: Emmanuel Macron’s marvellous manoeuvres this week - BBC News\n",
      "40% guardian, 40% telegraph, 20% independent\n",
      "\n",
      "Testing /news/world-asia-42729173\n",
      "New Zealand debates access to dead sea life footage - BBC News\n",
      "40% express, 40% daily mail, 20% telegraph\n",
      "\n",
      "Testing /news/entertainment-arts-42755887\n",
      "US musician Tom Petty died of 'accidental drug overdose' - BBC News\n",
      "60% daily mail, 20% guardian, 20% express\n",
      "\n",
      "Testing /news/world-us-canada-42709360\n",
      "Trumplomacy: Has Trump made the world more dangerous? - BBC News\n",
      "40% independent, 40% express, 20% guardian\n",
      "\n",
      "Testing /news/world-us-canada-42757091\n",
      "US shutdown begins as Senate fails to pass new budget - BBC News\n",
      "60% independent, 20% telegraph, 20% express\n",
      "\n",
      "Testing /news/world-latin-america-42757085\n",
      "Huge Brazil rubbish dump closes after six decades - BBC News\n",
      "40% independent, 40% daily mail, 20% guardian\n",
      "\n",
      "Testing /news/business-11428889\n",
      "Technology of Business - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/uk-england-sussex-42759124\n",
      "'Drunk' pilot removed from BA plane at Gatwick Airport - BBC News\n",
      "20% telegraph, 20% express, 20% independent, 20% daily mail, 20% guardian\n",
      "\n",
      "Testing /news/world-us-canada-42712855\n",
      "Women's March: Where, when and why will protests happen? - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42751715\n",
      "How can parents torture their children? - BBC News\n",
      "40% independent, 20% telegraph, 20% express, 20% guardian\n",
      "\n",
      "Testing /news/world-middle-east-42758532\n",
      "Syria conflict: 15 refugees found frozen to death - BBC News\n",
      "40% daily mail, 20% independent, 20% express, 20% telegraph\n",
      "\n",
      "Testing /news/uk-42708571\n",
      "Rebecca Loos says Max Clifford offered her £1m to do sex tape - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42758989\n",
      "US shutdown: Senate reacts to failure to agree on new budget - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-europe-42750584\n",
      "Why Italians are saying 'No' to takeaway coffee - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-europe-42758189\n",
      "Top French chef Paul Bocuse dies at 91 - BBC News\n",
      "40% express, 20% telegraph, 20% daily mail, 20% independent\n",
      "\n",
      "Testing /news/uk-politics-42757026\n",
      "Macron says 'special' UK deal possible - BBC News\n",
      "20% telegraph, 20% guardian, 20% daily mail, 20% independent, 20% express\n",
      "\n",
      "Testing /news/world-us-canada-42754608\n",
      "What happens to federal workers in a government shutdown? - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42759934\n",
      "US shutdown: Trump and Democrats blame each other - BBC News\n",
      "60% independent, 20% express, 20% telegraph\n",
      "\n",
      "Testing /news/world-us-canada-42667659\n",
      "The missing - consequences of Trump's immigration crackdown - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42740239\n",
      "Why are these women sticking with Trump? - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/world-us-canada-42739556\n",
      "A Stormy (Daniels) situation: Donald Trump's porn-star (non)-scandal - BBC News\n",
      "60% independent, 20% daily mail, 20% telegraph\n",
      "\n",
      "Testing /news/uk-england-london-42750634\n",
      "Celine Dookhran trial: Woman given '10 minutes to live' - BBC News\n",
      "60% independent, 20% daily mail, 20% telegraph\n",
      "\n",
      "Testing /news/world-europe-42758188\n",
      "Turkey bus crash: Eleven killed in motorway accident - BBC News\n",
      "40% daily mail, 20% independent, 20% express, 20% guardian\n",
      "\n",
      "Testing /news/world-us-canada-39698546\n",
      "US government shutdown: How did we get here? - BBC News\n",
      "60% independent, 20% express, 20% telegraph\n",
      "\n",
      "Testing /news/uk-42757023\n",
      "British tourists warned to 'stay in resorts' in Jamaica security emergency - BBC News\n",
      "40% guardian, 40% express, 20% telegraph\n",
      "\n",
      "Testing /news/world-42753653\n",
      "In case you missed it - BBC News\n",
      " -- No test body found, continuing...\n",
      "\n",
      "Testing /news/technology-42755832\n",
      "Facebook to use surveys to boost ‘trustworthy’ news - BBC News\n",
      "60% daily mail, 20% guardian, 20% independent\n",
      "\n",
      "Testing /news/uk-42758515\n",
      "Alan Carr gets married to long-term boyfriend in LA - BBC News\n",
      "40% telegraph, 40% express, 20% daily mail\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the BBC!\n",
    "bbc_k = 5\n",
    "count = 25 \n",
    "\n",
    "print('Getting BBC links...')\n",
    "bbc_soup  = soup_up('http://www.bbc.com/news')\n",
    "bbc_links = [ a['href'] for a in bbc_soup.find_all('a', { 'href': re.compile('^/news/(.*)\\d{6,}$') }) ]\n",
    "link_list = process_links( bbc_links, count )\n",
    "\n",
    "processed = 0;\n",
    "for test_link in link_list:\n",
    "    print('Testing ' + test_link )\n",
    "    test_soup = soup_up('http://www.bbc.com' + test_link)\n",
    "    print( test_soup.title.text )\n",
    "\n",
    "    test_body    = test_soup.find('div', {'property':'articleBody'})\n",
    "    if not test_body:\n",
    "        count += 1\n",
    "        print(' -- No test body found, continuing...\\n')\n",
    "        continue\n",
    "    test_para    = test_body.find_all('p', recursive=False )\n",
    "    if not test_para:\n",
    "        print(' -- No paragraphs found, continuing...\\n')\n",
    "        continue\n",
    "    \n",
    "    test_text   = ' '.join([ p.text for p in test_para ])\n",
    "    test_result = vs.test(test_text, word_count, bbc_k) \n",
    "    print(\", \".join([ '{}% {}'.format(int(c[1]/bbc_k*100), c[0]) for c in test_result ]))\n",
    "    print('')\n",
    "    \n",
    "    processed += 1\n",
    "    if( processed >= count ):\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
